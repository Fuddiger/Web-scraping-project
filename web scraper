import csv
import re
import requests
from bs4 import BeautifulSoup as bs
from datetime import datetime

#Creates a list of websites from which to scrape news article links
def get_sites():
    sites = ['https://globalnews.ca/montreal/', 'https://abcnews.go.com/', 'https://foxnews.com']
    return sites

#Returns a set for one site containing all the links for its news articles
def get_links(site):
    links = set()
    with requests.Session() as s:
        response = s.get(site)
        soup = bs(response.text,'lxml',)
        if site == 'https://globalnews.ca/montreal/':
            for tag in soup.find_all('a', href=re.compile('/news/')):
                links.add(tag['href'])
        elif site == 'https://abcnews.go.com':
            for tag in soup.find_all('a', href=re.compile('^https\S*\d{8}$')):
                links.add(tag['href'])
        elif site == 'https://foxnews.com':
            for tag in soup.find_all('a', href=re.compile('-[0-9|a-z]+-[0-9|a-z]+-[0-9|a-z]+$')):
                #A lot of links pulled from Fox News are missing the 'https:' at the beginning
                if re.search(r'^https', tag['href']):
                    links.add(tag['href'])
                else:
                    repaired_link = 'https:' + tag['href']
                    links.add(repaired_link)
    return links

#makes a BeautifulSoup object from a news article link
def link_soup(link):
    with requests.Session() as s:
        response = s.get(link)
        soup = bs(response.text, 'lxml')
        return soup

#Scrapes the publication date from an article formatted for the specific site and returns it as a date object
def scrape_date(soup):
    if site == 'https://globalnews.ca/montreal/':
        try:
            date = soup.find('span', string= re.compile('Posted')).get_text(strip=True)
            strp_date = datetime.strptime(date, 'Posted %B %d, %Y %I:%M %p')
        except:
            #Occasional articles have the date in a different place
            try:
                date = soup.find('div', class_='c-byline__date c-byline__published').get_text(strip=True)
                strp_date = datetime.strptime(date, 'Published %B %d, %Y')
            except:
                print(link)

    elif site == 'https://abcnews.go.com/':
        try:
            date = soup.find('div', class_='xTlfF Vxqj').get_text(separator=' ', strip=True)
            strp_date = datetime.strptime(date, '%B %d, %Y, %I:%M %p')
        except:
            print(link)
    elif site == 'https://foxnews.com':
        try:
            date = soup.find('div', class_='article-date').get_text(separator=' ',strip=True)
            strp_date = datetime.strptime(date, 'Published %B %d, %Y %I:%M%p EST')
        except:
            try:
                date = soup.find('div', class_='article-date').get_text(separator=' ',strip=True)
                strp_date = datetime.strptime(date, 'Updated on %B %d, %Y %I:%M%p EST')
            except:
                print(link)

#Returns the title of the article as a string
def scrape_title(soup):
    title = soup.title.get_text(strip=True)
    return title

#Returns the stripped article text delimited with spaces
def scrape_text(soup):
    text = soup.get_text(separator=' ', strip=True)
    return text

#Creates\Appends the data to a CSV file that will be created in the same folder as the program
def data_write(title, date, text):
    with open('scraper.txt', 'a', newline='', encoding='utf-8') as a:
        writer = csv.writer(a)
        writer.writerow([title])
        writer.writerow([date])
        writer.writerow([text, '\n'])

if __name__ == '__main__':
    sites = get_sites()
    for site in sites:
        links = get_links(site)
        for link in links:
            soup = link_soup(link)
            title = scrape_title(soup)
            date = scrape_date(soup)
            text = scrape_text(soup)
            data_write(title, date, text)

