import bs4
import csv
import re
import requests
from datetime import date

#Opens a CSV file or text file where each website is on one line, delimited by a comma
def get_links(file_path):
    with open(file_path, newline='') as f:
        reader = csv.reader(f)
        sites = []
        for row in reader:
            sites.append(row[0])
        return sites

#Makes a link into a beautiful soup
def make_soup(link):
    request = requests.get(link)
    soup = bs4.BeautifulSoup(request.text,'lxml',)
    return soup

#Various searches to find a publication date, new searches can be added as not found results come up. Returns a text string of the date or 'Not found'
def scrape_soup_date(soup):
    if soup.time:
        date = soup.time.string
        counter['path1'] += 1
        return date
    else:
        soup_text = soup.text
        if re.search(r'\d{1,4}(/|-)\d{1,2}(/|-)\d{1,4}',soup_text):
            date = re.search(r'\d{1,4}/\d{1,2}/\d{1,4}', soup_text).group()
            counter['path2'] += 1
            return date
        elif re.search(r'\w{3}\s\d{1,2},\s\d{1,4}',soup_text):
            date = re.search(r'\w{3,4}\s\d{1,2},\s\d{1,4}',soup_text).group()
            counter['path3'] += 1
            return date
        else:
            counter['not found'] += 1
            return 'Not found'

#Returns the text from <title>
def scrape_soup_title(soup):
    title = soup.find('title').get_text()
    return title

#Returns text from all <p> tags concatenated into a block of text
def scrape_soup_text(soup):
    paragraphs = soup.find_all('p')
    all_text = ''
    for paragraph in paragraphs:
        all_text += paragraph.get_text()
    return all_text

#Returns today's date, some publication dates come up as e.g. '2 hours ago (20:22 gmt)', so this date provides context
def get_the_date():
    date_object = date.today()
    return date_object

#Writes the data to a file to be processed later
def data_write(title, date, text):
    with open('scraper.txt', 'a', newline='', encoding='utf-8') as a:
        writer = csv.writer(a)
        writer.writerow([title])
        if date == 'Not found':
            writer.writerow(['Scraped: ', today, ' (publication date not found)'])
        else:
            writer.writerow([date])
        writer.writerow([text, '\n'])

#Place the full file link here with full websites delimited by commas
file = r'C:\Users\andre\OneDrive\Documents\Downloads\Files_for_scrape.txt'
#The websites from the file
websites = get_links(file)
#Getting the date
today = get_the_date()
#creating a counter for scrape_soup_date info
counter = {'path1': 0, 'path2': 0, 'path3': 0, 'not found': 0}
#Performs task for each link
for link in websites:
    soup = make_soup(link)
    title = scrape_soup_title(soup)
    date = scrape_soup_date(soup)
    text = scrape_soup_text(soup)
    data_write(title, date, text)
print(counter)